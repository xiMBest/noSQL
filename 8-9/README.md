# Lab 8-9

# Для початку встановимо потрібні компоненти в azure які нам знадобляться це:
  1. Databricks service
  ![redis](../screen/89_1.png)
  2. Обираємо підписку нашого акку, ресурс групу у якій ми будемо працювати, ім'я databricks'у, і найголовніше фрішний тір - після цього тиснем кнопку Review+create
  ![redis](../screen/89_2.png)
  3. Поки деплоїться датабрікс ми можемо створити Data Lake для цього в ажурі добавили Data Lake 2 у Storage account створюєм його та обираєм підписку, ім'я, а також Repliction LRS
  ![redis](../screen/89_3.png)
  4. Після цього найголовніший крок для створення Data lake нам потрібно перейти в відділ Advanced і тицьнути кнопку Enabled навпроти Data Lake storage Gen2 після цього Review+create
  ![redis](../screen/89_4.png)
  5. Далі нам потрібно створити аплікацію для цього вводимо у пошуку Azure Active Directory
  ![redis](../screen/89_5.png)
  6. Після цього у лівому меню нам потрібно обрати App Registration
  ![redis](../screen/89_6.png)
  7. Далі тиснем кнопку якщо ви вперше створюєте аплікацію вам висвітить кнопка реєстрація аплікації, якщо уже не вперше тоді просто New registration
  ![redis](../screen/89_7.png)
  8. Вводимо лише ім'я і створюєм апплікацію
  ![redis](../screen/89_8.png)
  9. Також нам потрібен буде EventHub його можна взяти з лабораторної роботи №5
  ![redis](../screen/89_9.png)
  10. Після цього переходимо у уже створений датабрікс і запускаєм його
  ![redis](../screen/89_10.png)
  11. Переходимо у clusters і створюємо новий кластер
  ![redis](../screen/89_11.png)
  12. При створенні кластеру обираєм усі поля як на скріні окрім ім'я
  ![redis](../screen/89_12.png)
  13. Після цього переходимо у наш кластер стартуєм його та переходимо до бібліотеки та тиснемо кнопку Install new
  ![redis](../screen/89_13.png)
  14. Тоді обираємо maven та вводимо в координати ось це ```com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.18```
  ![redis](../screen/89_14.png)
  15. Тепер переходимо у Workspace та створюємо Notebook python file та scala file
  ![redis](../screen/89_15.png)
  16. Після того як стартане кластер і заінсталиться наш мейвен переходимо у наші створені файли і вписуємо код з файлів mounty.py у пайтонівський файл та eventHub.scala у notebook scala
  ![redis](../screen/89_16.png)
  ![redis](../screen/89_16_1.png)
  17. Тепер повернемось в Azure та перейдем в наш Storage account у якому можна скачати програму для відображення директорій у нашому data lake, також він нам буде потрібен для створення додаткових прав на контейнер та директорію в ньому
  ![redis](../screen/89_17.png)
  18. Після цього потрібно створити Container
  ![redis](../screen/89_18.png)
  19. Тиснем кнопку + Контейнер та обираємо ім'я для контейнера та найголовніше додати йому публічні права та тиснем Create
  ![redis](../screen/89_19.png)
  20. Далі перейдемо у розділ Access Control у якому додамо роль контрібютора нашої аплікації так як показано на скріні не забуваємо зберегти!!
  ![redis](../screen/89_20.png)
  21. Тепер повернемось до нашої аплікації у якій нам будуть потрібні айдішки та сікрет
  ![redis](../screen/89_21.png)
  22. Перейдемо у сікрет та створимо його та зразу копіюємо так як він потім закриється
  ![redis](../screen/89_22.png)
  23. Також перейдем у програму яку ми встановили і надамо доступ контейнеру та папці у ньому нашій аплікаціїї
  ![redis](../screen/89_23.png)
  ![redis](../screen/89_23_1.png)
  ![redis](../screen/89_23_2.png)
  ![redis](../screen/89_23_3.png)
  ![redis](../screen/89_23_4.png)
  ![redis](../screen/89_23_5.png)
  ![redis](../screen/89_23_1.png)
  ![redis](../screen/89_23_2.png)
  ![redis](../screen/89_23_3.png)
  24. Далі нам потрібно повернутись у Databriks у пайтонівський файл у якому потрібно замінити ці поля а також вписати у рядку сорсу ім'я контейнера а також акаунту```source = "abfss://containername@accountname.dfs.core.windows.net/"```, а у рядку ```  mount_point = "/mnt/naida9/",  ``` директорії яку будемо маунтити
  ![redis](../screen/89_24.png)
  25. Далі ранимо наші джоби після маунтення директорії можемо переглянути чи все ок заранилось такою командою ```display(dbutils.fs.ls('/mnt/naida9'))```
  ![redis](../screen/89_25.png)
  26. Після цього нам потрібно перейти у код scala у якому нам потрібно вставити айдішки, конекшин стрінги та імена, тільки що ми їх вставляли у пайтонівський код client id - appid, secret - password, tenantID - tenantID, fileSystemName - container name, storageAccountName - ім'я аккаунт сторедж, connection string це connection string eventhub space, setEventHubName - ім'я eventHub у який ми записуємо дані, також connection string та ім'я івент хабу потрібно вставити у файл ```config.json```
  ![redis](../screen/89_26.png)
  ![redis](../screen/89_26_1.png)
  ![redis](../screen/89_26_2.png)
  27. Далі нам потрібно замінити поля які ми будемо парсити у файлики csv це потрібно змінити у функції filtered, а також у файлі ```EventHubWriter.py``` також можна додати ще поля чи забрати
  ![redis](../screen/89_27.png)
  ![redis](../screen/89_27_1.png)
  28. Далі ми змінюємо директорію у яку ми маунтели таку і нам вивело path у пункті 25 і вставляєм у рядок option та start
  ![redis](../screen/89_28.png)
  29. Далі раним стрім директорії і запускаєм програму на пайтоні а саме файл ```app.py```, та у програмі postman нам протрібно створити REST запит POST на лінку ```http://127.0.0.1:5000/eventhub``` також додати Connection-Type | application/json у хедері та у body прописати raw а саме урлу на наш датасет ```{"url": "<linkdataset.csv>"}```
  ![redis](../screen/89_29.png)
  30. Після того заходимо у програму від майкрософт яку ми скачували Microsoft Azure Storage Exporter у наш контейнер і дерикторію і обновляємо там почнуть з'являтись csv файли заповнені нашими полями які ми вказали в пункті 27, даними нашого датасету
  ![redis](../screen/89_30.png)
  ![redis](../screen/89_30_1.png)
